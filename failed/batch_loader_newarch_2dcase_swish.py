# -*- coding: utf-8 -*-
"""
Batch_Loader_newArch_2DCase_swish.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13fWDr5-gNxTxoEmjcCk1lgFFez3Af_tw
"""

import torch
import torch.nn as nn
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
import pandas as pd
import torch.optim as optim
import optuna
import time
import matplotlib.pyplot as plt
from matplotlib import cm
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Check if CUDA is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class Swish(nn.Module):
        def __init__(self, inplace=True):
            super(Swish, self).__init__()
            self.inplace = inplace

        def forward(self, x):
            if self.inplace:
                x.mul_(torch.sigmoid(x))
                return x
            else:
                return x * torch.sigmoid(x)



class PINN_u(nn.Module):

        #The __init__ function stack the layers of the
        #network Sequentially
        def __init__(self):
            super(PINN_u, self).__init__()
            self.main = nn.Sequential(
                nn.Linear(input_n,h_n),
                #nn.Tanh(),
                #nn.Sigmoid(),
                #nn.BatchNorm1d(h_n),
                Swish(),
                nn.Linear(h_n,h_n),
                #nn.Tanh(),
                #nn.Sigmoid(),
                #nn.BatchNorm1d(h_n),
                Swish(),
                nn.Linear(h_n,h_n),
                #nn.Tanh(),
                #nn.Sigmoid(),
                #nn.BatchNorm1d(h_n),
                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),
                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),


                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),


                Swish(),

                nn.Linear(h_n,1),
            )
        #This function defines the forward rule of
        #output respect to input.
        #def forward(self,x):
        def forward(self,x):
            output = self.main(x)
            return output

class PINN_v(nn.Module):

        #The __init__ function stack the layers of the
        #network Sequentially
        def __init__(self):
            super(PINN_v, self).__init__()
            self.main = nn.Sequential(
                nn.Linear(input_n,h_n),
                #nn.Tanh(),
                #nn.Sigmoid(),
                #nn.BatchNorm1d(h_n),
                Swish(),
                nn.Linear(h_n,h_n),
                #nn.Tanh(),
                #nn.Sigmoid(),
                #nn.BatchNorm1d(h_n),
                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),
                #nn.Tanh(),
                #nn.Sigmoid(),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),


                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),

                nn.Linear(h_n,1),
            )
        #This function defines the forward rule of
        #output respect to input.
        #def forward(self,x):
        def forward(self,x):
            output = self.main(x)
            return output

class PINN_w(nn.Module):

        #The __init__ function stack the layers of the
        #network Sequentially
        def __init__(self):
            super(PINN_w, self).__init__()
            self.main = nn.Sequential(
                nn.Linear(input_n,h_n),
                #nn.Tanh(),
                #nn.Sigmoid(),
                #nn.BatchNorm1d(h_n),
                Swish(),
                nn.Linear(h_n,h_n),
                #nn.Tanh(),
                #nn.Sigmoid(),
                #nn.BatchNorm1d(h_n),
                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),
                #nn.Tanh(),
                #nn.Sigmoid(),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),


                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),

                nn.Linear(h_n,1),
            )
        #This function defines the forward rule of
        #output respect to input.
        #def forward(self,x):
        def forward(self,x):
            output = self.main(x)
            return output

class PINN_p(nn.Module):

        #The __init__ function stack the layers of the
        #network Sequentially
        def __init__(self):
            super(PINN_p, self).__init__()
            self.main = nn.Sequential(
                nn.Linear(input_n,h_n),
                #nn.Tanh(),
                #nn.Sigmoid(),
                #nn.BatchNorm1d(h_n),
                Swish(),
                nn.Linear(h_n,h_n),
                #nn.Tanh(),
                #nn.Sigmoid(),
                #nn.BatchNorm1d(h_n),
                Swish(),
                nn.Linear(h_n,h_n),
                #nn.Tanh(),
                #nn.Sigmoid(),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),

                nn.Linear(h_n,1),
            )
        #This function defines the forward rule of
        #output respect to input.
        def forward(self,x):
            output = self.main(x)
            return  output

class PINN_T(nn.Module):

        #The __init__ function stack the layers of the
        #network Sequentially
        def __init__(self):
            super(PINN_T, self).__init__()
            self.main = nn.Sequential(
                nn.Linear(input_n,h_n),
                #nn.Tanh(),
                #nn.Sigmoid(),
                #nn.BatchNorm1d(h_n),
                Swish(),
                nn.Linear(h_n,h_n),
                #nn.Tanh(),
                #nn.Sigmoid(),
                #nn.BatchNorm1d(h_n),
                Swish(),
                nn.Linear(h_n,h_n),
                #nn.Tanh(),
                #nn.Sigmoid(),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),
                nn.Linear(h_n,h_n),

                #nn.BatchNorm1d(h_n),

                Swish(),

                nn.Linear(h_n,1),
            )
        #This function defines the forward rule of
        #output respect to input.
        def forward(self,x):
            output = self.main(x)
            return  output

class PINN_phi(nn.Module):

        #The __init__ function stack the layers of the
        #network Sequentially
        def __init__(self):
            super(PINN_phi, self).__init__()
            self.main = nn.Sequential(
                nn.Linear(input_n,h_n),                
                Swish(),
                nn.Linear(h_n,h_n),
                Swish(),
                nn.Linear(h_n,h_n),
                Swish(),
                nn.Linear(h_n,h_n),
                Swish(),
                nn.Linear(h_n,h_n),
                Swish(),
                nn.Linear(h_n,h_n),
                Swish(),
                nn.Linear(h_n,h_n),
                Swish(),
                nn.Linear(h_n,h_n),
                Swish(),
                nn.Linear(h_n,h_n),
                Swish(),
                nn.Linear(h_n,h_n),
                Swish(),
                nn.Linear(h_n,1),
            )
        #This function defines the forward rule of
        #output respect to input.
        def forward(self,x):
            output = self.main(x)
            return  output

################################################################

def init_normal(m):
		if type(m) == nn.Linear:
			nn.init.kaiming_normal_(m.weight)
   
def init_xavier(m):
    if isinstance(m, nn.Linear):  # Check if the layer is nn.Linear
        nn.init.xavier_normal_(m.weight)  # Xavier normal distribution
        if m.bias is not None:           # Initialize biases to zero if present
            nn.init.zeros_(m.bias)


#PINN_u is model(networks for each components)
input_n = 2
h_n = 40
PINN_u = PINN_u().to(device)
PINN_v = PINN_v().to(device)
PINN_w = PINN_w().to(device)
PINN_p = PINN_p().to(device)
PINN_T = PINN_T().to(device)
PINN_phi = PINN_phi().to(device)

PINN_u.apply(init_xavier)
PINN_v.apply(init_xavier)
PINN_p.apply(init_xavier)
PINN_T.apply(init_xavier)
PINN_phi.apply(init_xavier)


###################################################################

def pde_residuals(PINN_u , PINN_v , PINN_p , PINN_T , x , y ):

    u = PINN_u(torch.cat((x,y) , dim = 1))
    v = PINN_v(torch.cat((x,y) , dim = 1))
    p = PINN_p(torch.cat((x,y) , dim = 1))
    T = PINN_T(torch.cat((x,y) , dim = 1))
    phi = PINN_phi(torch.cat((x,y) , dim = 1))

    phi_y = torch.autograd.grad(phi, y, grad_outputs=torch.ones_like(phi), create_graph=True)[0]
    phi_x = torch.autograd.grad(phi, x, grad_outputs=torch.ones_like(phi), create_graph=True)[0]

    #u = phi_y  # u = ∂φ/∂y
    #v = -phi_x # v = -∂φ/∂x



    # Calculate gradients
    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]
    u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), create_graph=True)[0]


    v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True)[0]
    v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(v), create_graph=True)[0]

    omega = v_x - u_y #vortisity scalar

    omega_x = torch.autograd.grad(omega, x, grad_outputs=torch.ones_like(omega), create_graph=True)[0]
    omega_y = torch.autograd.grad(omega, y, grad_outputs=torch.ones_like(omega), create_graph=True)[0]

    omega_xx = torch.autograd.grad(omega_x, x, grad_outputs=torch.ones_like(omega_x), create_graph=True)[0]
    omega_yy = torch.autograd.grad(omega_x, y, grad_outputs=torch.ones_like(omega_y), create_graph=True)[0]


    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]
    u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), create_graph=True)[0]


    v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), create_graph=True)[0]
    v_yy = torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(v_y), create_graph=True)[0]


    p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(p), create_graph=True)[0]
    p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(p), create_graph=True)[0]


    T_x = torch.autograd.grad(T, x, grad_outputs=torch.ones_like(T), create_graph=True)[0]
    T_y = torch.autograd.grad(T, y, grad_outputs=torch.ones_like(T), create_graph=True)[0]

    T_xx = torch.autograd.grad(T_x, x, grad_outputs=torch.ones_like(T_x), create_graph=True)[0]
    T_yy = torch.autograd.grad(T_x, y, grad_outputs=torch.ones_like(T_x), create_graph=True)[0]


    # Continuity equation
    continuity_residual = u_x + v_y

    # Momentum equations
    nu = 0.01 #kinematic viscosity
    alpha = 0.02	#Diffusivity
    momentum_u_residual = (u * u_x + v * u_y ) + p_x - nu * (u_xx + u_yy)
    momentum_v_residual = (u * v_x + v * v_y ) + p_y - nu * (v_xx + v_yy)
    # Compute convective term: u ∂ω/∂x + v ∂ω/∂y   -  Diffusive term: ν (∂²ω/∂x² + ∂²ω/∂y²)
    #omega_residual = (u * omega_x + v * omega_y ) - nu * (omega_xx + omega_yy)

    # Energy Equation
    energy_residual = u * T_x + v * T_y  - alpha * (T_xx + T_yy ) #- alpha_t_diff
    loss_mse = nn.MSELoss()
    #Note our target is zero. It is residual so we use zeros_like
    loss_pde = loss_mse(continuity_residual,torch.zeros_like(continuity_residual)) + loss_mse(momentum_u_residual,torch.zeros_like(momentum_u_residual)) + loss_mse(momentum_v_residual,torch.zeros_like(momentum_v_residual)) + loss_mse(energy_residual,torch.zeros_like(energy_residual))
    #loss_pde = loss_mse(continuity_residual,torch.zeros_like(continuity_residual)) + loss_mse(omega_residual,torch.zeros_like(omega_residual)) + loss_mse(energy_residual,torch.zeros_like(energy_residual))
    return loss_pde



def loader(filename_data , b_size):


  data = pd.read_csv(filename_data)
  data = 2 * (data - data.min()) / (data.max() - data.min()) - 1
  # Extract input and label columns (replace 'x', 'y', 'u', 'v' with your actual column names)
  inputs = (data[['x', 'y']].values).reshape(-1 ,2)
  label_u = (data[['u']].values).reshape(-1 , 1)
  label_v = (data[['v']].values).reshape(-1 , 1)
  label_T = (data[['T']].values).reshape(-1 , 1)
  label_p = (data[['p']].values).reshape(-1 , 1)


  # Convert normalized data into PyTorch tensors
  inputs_tensor = torch.tensor(inputs, dtype=torch.float32)
  label_u_tensor = torch.tensor(label_u, dtype=torch.float32)
  label_v_tensor = torch.tensor(label_v, dtype=torch.float32)
  label_T_tensor = torch.tensor(label_T, dtype=torch.float32)
  label_p_tensor = torch.tensor(label_p, dtype=torch.float32)

  # Create a TensorDataset
  dataset = TensorDataset(inputs_tensor, label_u_tensor , label_v_tensor , label_T_tensor , label_p_tensor)

  # Define your batch_size
  # b_size is  : Batch_size
  dataloader = DataLoader(dataset, batch_size=b_size, shuffle=True)

  return dataloader




def loader_loss(PINN_u , PINN_v , PINN_p , PINN_T , f_name , batch_size):

  loss_mse = nn.MSELoss()
  dloader = loader(f_name , batch_size)
  inputs, u_labels, v_labels , T_labes , p_labels = next(iter(dloader))

  inputs = inputs.to(device)
  u_labels = u_labels.to(device)
  v_labels = v_labels.to(device)
  T_labes = T_labes.to(device)
  p_labels = p_labels.to(device)

  u_data_pred = PINN_u(inputs)
  v_data_pred = PINN_u(inputs)
  T_data_pred = PINN_p(inputs)
  p_data_pred = PINN_p(inputs)

  u_data_loss = loss_mse(u_data_pred, u_labels)
  v_data_loss = loss_mse(v_data_pred, v_labels)
  T_data_loss = loss_mse(T_data_pred, T_labes)
  p_data_loss = loss_mse(p_data_pred, p_labels)
  loss_data = u_data_loss + v_data_loss + T_data_loss + p_data_loss
  return loss_data

def total_loss(PINN_u , PINN_v , PINN_p , PINN_T ,
          filename_data , filename_bc ,
          x_c , y_c , lambda_pde , lambda_data , lambda_bc , batch_size):
  loss_data = loader_loss(PINN_u , PINN_v , PINN_p , PINN_T , filename_data , batch_size)
  loss_bc = loader_loss(PINN_u , PINN_v , PINN_p , PINN_T , filename_bc , batch_size)
  loss_pde = pde_residuals(PINN_u , PINN_v , PINN_p , PINN_T , x_c , y_c)
  loss = lambda_pde * loss_pde + lambda_data * loss_data + lambda_bc * loss_bc
  return loss , loss_data * lambda_data , loss_bc * lambda_bc , loss_pde *lambda_pde

def plot_loader(f_name , title , PINN_u , PINN_v , PINN_T , PINN_p):
  PINN_u.eval()
  PINN_v.eval()
  PINN_T.eval()
  PINN_p.eval()

  filename_data = f_name
  dloader = loader(filename_data , 64)
  inputs, u_labels, v_labels , T_labes , p_labels = next(iter(dloader))

  inputs = inputs.to(device)
  u_labels = u_labels.to(device)
  v_labels = v_labels.to(device)
  T_labes = T_labes.to(device)
  p_labels = p_labels.to(device)

  u_data_pred = PINN_u(inputs)
  v_data_pred = PINN_u(inputs)
  T_data_pred = PINN_p(inputs)
  p_data_pred = PINN_p(inputs)

  fig, axs = plt.subplots(2 , 1, sharex=True, sharey=True)
  axs[0].plot(u_labels.cpu().detach().numpy() , ls = "-", marker = 'o' , label = "Truth")
  axs[0].plot(u_data_pred.cpu().detach().numpy() , 'x' , label = "PINN")
  axs[0].set_title('u')
  axs[0].set_ylabel('u/Uref')

  axs[1].plot(v_labels.cpu().detach().numpy() ,ls = "-" ,  marker = 'o' , label = "Truth")
  axs[1].plot(v_data_pred.cpu().detach().numpy() , 'x' , label = "PINN")
  axs[1].set_title('v')
  axs[1].set_ylabel('v/Uref')

  plt.suptitle(title)
  plt.legend(loc = 'best')
  plt.show()


def train(PINN_u , PINN_v , PINN_p , PINN_T ,
          filename_data , filename_bc , filename_test ,
          x_c , y_c , lambda_pde , lambda_data , lambda_bc  , batch_size):
  def objective(trial):

    # Hyperparameters for tuning
    lambda_pde = trial.suggest_float("lambda_pde", 0, 0.01)
    lambda_data = trial.suggest_float("lambda_data", 0.0, 0.010)
    lambda_bc = trial.suggest_float("lambda_bc", 0.0, 0.010)


    opt_u_adam = optim.Adam(PINN_u.parameters() , lr = 1e-2)
    opt_v_adam = optim.Adam(PINN_v.parameters() , lr = 1e-2)
    opt_p_adam = optim.Adam(PINN_u.parameters() , lr = 1e-2)
    opt_T_adam = optim.Adam(PINN_T.parameters() , lr = 1e-2)

    num_epochs = 50  #best value is 500
    for epoch in range(num_epochs):
      opt_u_adam.zero_grad()
      opt_v_adam.zero_grad()
      opt_p_adam.zero_grad()
      opt_T_adam.zero_grad()
      loss , loss_data , loss_bc , loss_pde = total_loss(PINN_u , PINN_v , PINN_p , PINN_T ,
          filename_data , filename_bc ,
          x_c , y_c , lambda_pde , lambda_data , lambda_bc , batch_size)

      loss.backward()
      opt_u_adam.step()
      opt_v_adam.step()
      opt_p_adam.step()
      opt_T_adam.step()

      # Return the final loss for this trial
      return loss.item()

  num_trials= 50
  # Run the Optuna hyperparameter optimization
  study = optuna.create_study(direction="minimize")
  study.optimize(objective, n_trials = num_trials)  # Adjust n_trials for more thorough search

  # Extract the best lambda values
  best_params = study.best_params
  print("Optimized lambda_pde:", best_params["lambda_pde"])
  print("Optimized lambda_data:", best_params["lambda_data"])

  lambda_pde = best_params["lambda_pde"]
  lambda_data = best_params["lambda_data"]
  lambda_bc = best_params["lambda_bc"]


  opt_u_adam = optim.Adam(PINN_u.parameters() , lr = 1e-2)
  opt_v_adam = optim.Adam(PINN_v.parameters() , lr = 1e-2)
  opt_p_adam = optim.Adam(PINN_p.parameters() , lr = 1e-2)
  opt_T_adam = optim.Adam(PINN_T.parameters() , lr = 1e-2)



  scheduler_u = ReduceLROnPlateau(opt_u_adam , factor = 0.5 , min_lr = 1e-3 , verbose=False )
  scheduler_v = ReduceLROnPlateau(opt_v_adam , factor = 0.5 , min_lr = 1e-3 , verbose=False )
  scheduler_p = ReduceLROnPlateau(opt_p_adam , factor = 0.5 , min_lr = 1e-3 , verbose=False )
  scheduler_T = ReduceLROnPlateau(opt_T_adam , factor = 0.5 , min_lr = 1e-3 , verbose=False )

  opt_u_lbfgs=torch.optim.LBFGS(PINN_u.parameters(),
                                   lr=0.1,  # or adjust based on your problem
                                   max_iter=100,  # More iterations for better convergence
                                   max_eval=None,  # Default
                                   tolerance_grad=1e-7,  # Increase sensitivity to gradients
                                   tolerance_change=1e-9,  # Keep default unless facing early stops
                                   history_size=100,  # Use larger history for better approximations
                                   line_search_fn="strong_wolfe")  # Use strong Wolfe line search for better convergence


  opt_v_lbfgs=torch.optim.LBFGS(PINN_v.parameters(),
                                   lr=0.1,  # or adjust based on your problem
                                   max_iter=100,  # More iterations for better convergence
                                   max_eval=None,  # Default
                                   tolerance_grad=1e-7,  # Increase sensitivity to gradients
                                   tolerance_change=1e-9,  # Keep default unless facing early stops
                                   history_size=100,  # Use larger history for better approximations
                                   line_search_fn="strong_wolfe")  # Use strong Wolfe line search for better convergence

  opt_p_lbfgs=torch.optim.LBFGS(PINN_p.parameters(),
                                   lr=0.1,  # or adjust based on your problem
                                   max_iter=100,  # More iterations for better convergence
                                   max_eval=None,  # Default
                                   tolerance_grad=1e-7,  # Increase sensitivity to gradients
                                   tolerance_change=1e-9,  # Keep default unless facing early stops
                                   history_size=100,  # Use larger history for better approximations
                                   line_search_fn="strong_wolfe")  # Use strong Wolfe line search for better convergence

  opt_T_lbfgs=torch.optim.LBFGS(PINN_T.parameters(),
                                   lr=0.1,  # or adjust based on your problem
                                   max_iter=100,  # More iterations for better convergence
                                   max_eval=None,  # Default
                                   tolerance_grad=1e-7,  # Increase sensitivity to gradients
                                   tolerance_change=1e-9,  # Keep default unless facing early stops
                                   history_size=100,  # Use larger history for better approximations
                                   line_search_fn="strong_wolfe")  # Use strong Wolfe line search for better convergence

  tic = time.time()

  def closure():
    opt_u_lbfgs.zero_grad()
    opt_v_lbfgs.zero_grad()
    opt_p_lbfgs.zero_grad()
    opt_T_lbfgs.zero_grad()


    loss , loss_data , loss_bc , loss_pde = total_loss(PINN_u , PINN_v , PINN_p , PINN_T ,
          filename_data , filename_bc ,
          x_c , y_c , lambda_pde , lambda_data , lambda_bc , batch_size)

    loss.backward()
    return loss

  hist_val = []



  loss_mse = nn.MSELoss()

  internal_dataloader = loader(filename_data , 64)
  boundary_dataloader  = loader(filename_bc , 64)

  epoch_adam = 10000
  epoch_lbgfs = 500

  PINN_u.train()
  PINN_v.train()
  PINN_p.train()
  PINN_T.train()

  # loop Adam
  for epo in range(epoch_adam):
    for batch_idx ,  batch in enumerate(internal_dataloader):
    #for (internal_inputs, u_data , v_data , T_data , p_data), (boundary_inputs, u_bc , v_bc , T_bc , p_bc) in zip(internal_dataloader, boundary_dataloader):
    #for batch_idx  in internal_dataloader:
      # Zero out the gradients
      opt_u_adam.zero_grad()
      opt_v_adam.zero_grad()
      opt_p_adam.zero_grad()
      opt_T_adam.zero_grad()
      loss , loss_data , loss_bc , loss_pde = total_loss(PINN_u , PINN_v , PINN_p , PINN_T ,
          filename_data , filename_bc ,
          x_c , y_c , lambda_pde , lambda_data , lambda_bc , batch_size)

      loss.backward()

      hist_val.append(loss.item())

      opt_u_adam.step()
      opt_v_adam.step()
      opt_p_adam.step()
      opt_T_adam.step()

      scheduler_u.step(loss)
      scheduler_v.step(loss)
      scheduler_p.step(loss)
      scheduler_T.step(loss)

    if epo %500 == 0:
        print(f"==========================================Batch index is : {batch_idx}==============================================")
        print(f'Epoch Adam {epo}/{epoch_adam} [{100 * epo/epoch_adam :.2f}%]  ,Total Loss: {loss.item():.6f} , Learning Rate is: {scheduler_u.get_last_lr() + scheduler_v.get_last_lr() + scheduler_p.get_last_lr() + scheduler_T.get_last_lr()}')
        print(f'PDE Loss {loss_pde:.4f} | Data loss {loss_data:.4f} | Boundary condition loss {loss_bc:.4f}')
        print(f"====================== End Batch INDEX {batch_idx} =================================================================")
        
        plot_loader(filename_data , f"train {batch_idx}" , PINN_u , PINN_v , PINN_T , PINN_p)
        

    if loss.item() <=0.00001 :
        print("Optimization Method is switching to LBGF-S . . . ")
        plot_loader(filename_test , "test" , PINN_u , PINN_v , PINN_T , PINN_p)
        break

    # loop LBFGS
  for epo in range(epoch_lbgfs):

    for batch_idx ,  batch in enumerate(internal_dataloader):
      loss_u = opt_u_lbfgs.step(closure)
      loss_v = opt_u_lbfgs.step(closure)
      loss_p = opt_p_lbfgs.step(closure)
      loss_T = opt_T_lbfgs.step(closure)
      loss = (loss_u + loss_v + loss_p + loss_T) / 4.0
      hist_val.append(loss.item())

    if epo % 10 == 0:
        
        print(f"==========================================Batch index is : {batch_idx}==============================================")
        print(f'Epoch Adam {epo}/{epoch_lbgfs} [{100 * epo/epoch_lbgfs :.2f}%]  ,Total Loss: {loss.item():.6f} , Learning Rate is: {scheduler_u.get_last_lr() + scheduler_v.get_last_lr() + scheduler_p.get_last_lr() + scheduler_T.get_last_lr()}')
        print(f'PDE Loss {loss_pde:.4f} | Data loss {loss_data:.4f} | Boundary condition loss {loss_bc:.4f}')
        print("=====================================================================================================================")
    # Assuming model, optimizer, and current epoch are defined
    checkpoint = {
    'epoch': epoch,
    'PINN_U_state_dict': PINN_u.state_dict(),
    'PINN_v_state_dict': PINN_v.state_dict(),
    'PINN_T_state_dict': PINN_T.state_dict(),
    'PINN_p_state_dict': PINN_p.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'hyperparameters': {'learning_rate': 0.001, 'batch_size': 64}  # Example hyperparams
        }

    # Save to a file
    torch.save(checkpoint, 'Results\pinn_model_checkpoint.pth')
    
    
        


# Define domain boundaries
ub = torch.tensor([ 10, 5])
lb = torch.tensor([ 0 , 0])


# Number of points in each dimension
n_points = 500  #collocation points number


# Random sampling points in the domain
x = np.random.uniform(lb[0], ub[0], n_points)
y = np.random.uniform(lb[1], ub[1], n_points)
np.random.seed(50)


# Convert to PyTorch tensors for use in the PINN model
x_c = torch.tensor(x, dtype=torch.float32, requires_grad=True).view(-1, 1)
y_c = torch.tensor(y, dtype=torch.float32, requires_grad=True).view(-1, 1)

#Balancing theses numberes are very cruisal
lambda_pde =  1 #10
lambda_data = 1 #4
lambda_bc = 1 #0.05

epoch_lbgfs = 120
epoch_adam = 13000
batch_size = 64

filename_data = r'2D_newData.csv'
filename_bc =  r'BC_data_2D_Lamin.csv'
filename_test = r"2D_newTest.csv"



train(PINN_u , PINN_v , PINN_p , PINN_T ,
          filename_data , filename_bc , filename_test ,
          x_c , y_c , lambda_pde , lambda_data , lambda_bc , batch_size)



#loading mdoel
# Assuming `model` has been loaded as in previous steps
PINN_u.eval()  # Set to evaluation mode
PINN_v.eval()
PINN_T.eval()
PINN_p.eval()



# Sample new data, e.g., a tensor of x, y, z coordinates
# Replace this with your actual new input data
#new_data = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=torch.float32)  # Shape: [num_samples, input_dim]
new_data = new_data.to('cuda' if torch.cuda.is_available() else 'cpu')  # Send to the appropriate device

# Make predictions
with torch.no_grad():  # Disable gradient tracking for faster inference
    predictions = model(new_data)

# Convert predictions back to CPU if needed and detach from computation graph
predictions = predictions.cpu().detach().numpy()
print(predictions)